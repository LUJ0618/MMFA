import math
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import LayerNorm
from baseline.WavLM import *

import torch
import torch.nn as nn

import matplotlib.pyplot as plt
import seaborn as sns

class ASPM(nn.Module):
    def __init__(self, input_dim):
        super(ASPM, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x, mask_ratio=0.5):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
            mask_ratio: Float, proportion of frames to mask (e.g., 0.3 for 30%)
        Returns:
            masked_output: Tensor of shape (batch_size, time_steps, feature_dim)
        """
        # Linear transformation U^T * h_t + p
        scores = self.linear(x)  # Shape: (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)  # Apply tanh activation

        # Attention score s_t = v^T * scores + q
        scores = self.attention_vector(scores).squeeze(-1)  # Shape: (batch_size, time_steps)

        # Sort scores to get indices of the lowest scores
        batch_size, time_steps = scores.shape
        num_mask_frames = int(time_steps * mask_ratio)

        sorted_scores, sorted_indices = torch.sort(scores, dim=1)
        mask_indices = sorted_indices[:, :num_mask_frames]  # Indices of the lowest scores

        # Create a mask tensor
        mask = torch.ones_like(scores, dtype=torch.float32)  # Start with all ones
        for i in range(batch_size):
            mask[i, mask_indices[i]] = 0  # Set the lowest-scoring frames to zero

        # Apply the mask: Set masked scores to a very small value (-inf)
        scores = torch.where(mask.bool(), scores, torch.full_like(scores, float('-inf')))

        # Softmax to compute attention weights Œ±_t
        masked_attention_weights = F.softmax(scores, dim=1)  # Shape: (batch_size, time_steps)

        # üîπ 0 Í∞í Í∞úÏàò Í≥ÑÏÇ∞ Î∞è Ï∂úÎ†•
        # üîπ ÌñâÎ≥Ñ(ÌîÑÎ†àÏûÑÎ≥Ñ)Î°ú 0 Í∞úÏàòÎ•º Í≥ÑÏÇ∞ÌïòÏó¨ Ï∂úÎ†•
        # zero_counts_per_row = (masked_attention_weights == 0).sum(dim=1)

        # for row_idx, zero_count in enumerate(zero_counts_per_row.tolist()):  # `.tolist()`Î°ú Î≥ÄÌôòÌïòÏó¨ Ï∂úÎ†•
        #     print(f"üîπ Speaker {row_idx}: {zero_count} zero values")
            
        # num_zeros = (masked_attention_weights == 0).sum().item()
        # print(f"üîπ Masked Attention Weights - Zero Count: {num_zeros}")

        # Multiply attention weights with the input embeddings
        masked_output = x * masked_attention_weights.unsqueeze(-1)  # Shape: (batch_size, time_steps, feature_dim)

        return masked_output, masked_attention_weights

class ASPMSoftmax(nn.Module):
    def __init__(self, input_dim):
        super(ASPMSoftmax, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x, mask_ratio=0.7):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
            mask_ratio: Float, proportion of frames to mask (e.g., 0.3 for 30%)
        Returns:
            masked_output: Tensor of shape (batch_size, time_steps, feature_dim)
        """
        # Linear transformation U^T * h_t + p
        scores = self.linear(x)  # Shape: (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)  # Apply tanh activation

        # Attention score s_t = v^T * scores + q
        scores = self.attention_vector(scores).squeeze(-1)  # Shape: (batch_size, time_steps)

        # Softmax to compute attention weights Œ±_t
        attention_weights = F.softmax(scores, dim=1)  # Shape: (batch_size, time_steps)

        # Sort scores to get indices of the lowest scores (for masking)
        batch_size, time_steps = scores.shape
        num_mask_frames = int(time_steps * mask_ratio)

        sorted_scores, sorted_indices = torch.sort(scores, dim=1)
        mask_indices = sorted_indices[:, :num_mask_frames]  # Indices of the lowest scores

        # Create a mask tensor (default: all ones)
        mask = torch.ones_like(attention_weights, dtype=torch.float32)  # Shape: (batch_size, time_steps)
        for i in range(batch_size):
            mask[i, mask_indices[i]] = 0  # Set the lowest-scoring frames to zero

        # Apply the mask: Set masked attention weights to zero
        masked_attention_weights = attention_weights * mask  # Shape: (batch_size, time_steps)

        # Normalize again to ensure sum of attention weights is valid (prevent division by zero)
        # masked_attention_weights = masked_attention_weights / masked_attention_weights.sum(dim=1, keepdim=True).clamp(min=1e-6)

        # Multiply attention weights with the input embeddings
        masked_output = x * masked_attention_weights.unsqueeze(-1)  # Shape: (batch_size, time_steps, feature_dim)

        return masked_output, masked_attention_weights

class ASPMSoftMasking(nn.Module):
    def __init__(self, input_dim):
        super(ASPMSoftMasking, self).__init__()
        self.input_dim = input_dim

        # Linear transformation layers for attention score calculation
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

        # Learnable vector H0 for soft masking
        self.H0 = nn.Parameter(torch.randn(input_dim))  # Learnable parameter for H0

    def forward(self, x, mask_ratio=0.7):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
        Returns:
            masked_output: Tensor of shape (batch_size, time_steps, feature_dim)
            masked_attention_weights: Tensor of shape (batch_size, time_steps)
        """
        batch_size, time_steps, feature_dim = x.shape

        # Linear transformation U^T * h_t + p
        scores = self.linear(x)  # Shape: (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)  # Apply tanh activation

        # Attention score s_t = v^T * scores + q
        scores = self.attention_vector(scores).squeeze(-1)  # Shape: (batch_size, time_steps)

        # Softmax to compute attention weights Œ±_t
        attention_weights = F.softmax(scores, dim=1)  # Shape: (batch_size, time_steps)

        # Sort scores to get indices of the lowest scores (for masking)
        num_mask_frames = int(time_steps * mask_ratio)
        sorted_scores, sorted_indices = torch.sort(scores, dim=1)
        mask_indices = sorted_indices[:, :num_mask_frames]  # Indices of the lowest scores

        # Create a mask tensor (default: all ones)
        mask = torch.ones_like(attention_weights, dtype=torch.float32)  # Shape: (batch_size, time_steps)
        for i in range(batch_size):
            mask[i, mask_indices[i]] = 0  # Set the lowest-scoring frames to zero

        # Apply the mask to attention weights (Soft Masking)
        masked_attention_weights = attention_weights * mask  # Shape: (batch_size, time_steps)

        # Soft Masking: Apply learnable vector H0 when mask is 0
        H0_expanded = self.H0.unsqueeze(0).unsqueeze(0).expand(batch_size, time_steps, feature_dim)
        masked_output = x * masked_attention_weights.unsqueeze(-1) + (1 - masked_attention_weights.unsqueeze(-1)) * H0_expanded

        return masked_output, masked_attention_weights

class ASPMResidual(nn.Module):
    def __init__(self, input_dim):
        super(ASPMResidual, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x, mask_ratio=0.5):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
            mask_ratio: Float, proportion of frames to mask (e.g., 0.3 for 30%)
        Returns:
            residual_output: Tensor of shape (batch_size, time_steps, feature_dim)
        """
        # Linear transformation U^T * h_t + p
        scores = self.linear(x)  # Shape: (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)  # Apply tanh activation

        # Attention score s_t = v^T * scores + q
        scores = self.attention_vector(scores).squeeze(-1)  # Shape: (batch_size, time_steps)

        # Softmax to compute attention weights Œ±_t
        attention_weights = F.softmax(scores, dim=1)  # Shape: (batch_size, time_steps)

        # Sort scores to get indices of the lowest scores (for masking)
        batch_size, time_steps = scores.shape
        num_mask_frames = int(time_steps * mask_ratio)

        sorted_scores, sorted_indices = torch.sort(scores, dim=1)
        mask_indices = sorted_indices[:, :num_mask_frames]  # Indices of the lowest scores

        # Create a mask tensor (default: all ones)
        mask = torch.ones_like(attention_weights, dtype=torch.float32)  # Shape: (batch_size, time_steps)
        for i in range(batch_size):
            mask[i, mask_indices[i]] = 0  # Set the lowest-scoring frames to zero

        # Apply the mask: Set masked attention weights to zero
        masked_attention_weights = attention_weights * mask  # Shape: (batch_size, time_steps)

        # Residual Î∞©Ïãù Ï†ÅÏö©: Í∏∞Î≥∏ Ï†ïÎ≥¥ Ïú†ÏßÄÌïòÎ©¥ÏÑú Í∞ÄÏ§ëÏπò Î∞òÏòÅ
        residual_output = x + masked_attention_weights.unsqueeze(-1) * x  # Shape: (batch_size, time_steps, feature_dim)

        return residual_output, masked_attention_weights

class ASP(nn.Module):
    def __init__(self, input_dim):
        super(ASP, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x):
        """
        Args:
            x: (batch_size, time_steps, feature_dim)
        Returns:
            pooled_output: (batch_size, feature_dim)
            attention_weights: (batch_size, time_steps)
        """
        # 1) ÏÑ†Ìòï Î≥ÄÌôò Î∞è tanh
        scores = self.linear(x)               # (B, T, D)
        scores = torch.tanh(scores)           # (B, T, D)

        # 2) Ïä§ÏπºÎùº Ïñ¥ÌÖêÏÖò Ïä§ÏΩîÏñ¥ Í≥ÑÏÇ∞ (v^T)
        scores = self.attention_vector(scores).squeeze(-1)  # (B, T)

        # 3) Softmax Î°ú ÌîÑÎ†àÏûÑÎ≥Ñ Í∞ÄÏ§ëÏπò ÏÇ∞Ï∂ú
        attention_weights = F.softmax(scores, dim=1)        # (B, T)

        # 4) Í∞ÄÏ§ëÌï© (Weighted Sum) ‚Üí (B, D)
        pooled_output = torch.sum(
            x * attention_weights.unsqueeze(-1), 
            dim=1
        )

        return pooled_output, attention_weights

class ASPMRandom(nn.Module):
    def __init__(self, input_dim):
        super(ASPMRandom, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x, mask_ratio=0.5):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
            mask_ratio: Float, Ï†ÑÏ≤¥ ÌîÑÎ†àÏûÑ Ï§ë ÎßàÏä§ÌÇπÌï† ÎπÑÏú® (Ïòà: 0.3Ïù¥Î©¥ 30%Î•º Î¨¥ÏûëÏúÑÎ°ú ÎßàÏä§ÌÇπ)
        Returns:
            masked_output: Tensor of shape (batch_size, time_steps, feature_dim)
            masked_attention_weights: Tensor of shape (batch_size, time_steps)
        """
        # 1) ÏÑ†Ìòï Î≥ÄÌôòÍ≥º tanh Ï†ÅÏö©
        scores = self.linear(x)                    # (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)                # tanh ÌôúÏÑ±Ìôî
        scores = self.attention_vector(scores).squeeze(-1)  # (batch_size, time_steps)

        # 2) Î¨¥ÏûëÏúÑ ÎßàÏä§ÌÇπÌï† Ïù∏Îç±Ïä§ ÏÑ†ÌÉù
        batch_size, time_steps = scores.shape
        num_mask_frames = int(time_steps * mask_ratio)  # ÎßàÏä§ÌÇπÌï† ÌîÑÎ†àÏûÑ Ïàò

        # Î™®Îì† ÏúÑÏπòÎ•º 1Î°ú Ï¥àÍ∏∞ÌôîÌïú mask ÌÖêÏÑú
        mask = torch.ones_like(scores, dtype=torch.float32)  # (batch_size, time_steps)

        for i in range(batch_size):
            # randpermÏùÑ Ïù¥Ïö©ÌïòÏó¨ Î¨¥ÏûëÏúÑ ÌîÑÎ†àÏûÑ Ïù∏Îç±Ïä§ Ï∂îÏ∂ú
            rand_indices = torch.randperm(time_steps)[:num_mask_frames]
            mask[i, rand_indices] = 0  # ÎßàÏä§ÌÇπÌï† ÏúÑÏπòÏóê 0ÏùÑ ÏÑ§Ï†ï

        # 3) ÏÜåÌîÑÌä∏Îß•Ïä§ ‚Üí Ïñ¥ÌÖêÏÖò Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
        attention_weights = F.softmax(scores, dim=1)  # (batch_size, time_steps)

        # 4) ÎßàÏä§ÌÇπÏù¥ 0Ïù∏ Í≥≥Ïóê Ïñ¥ÌÖêÏÖò Í∞ÄÏ§ëÏπò 0ÏúºÎ°ú ÎçÆÏñ¥ÏîåÏö∞Í∏∞
        masked_attention_weights = attention_weights * mask  # (batch_size, time_steps)

        # 5) ÏûÖÎ†• xÏóê ÎßàÏä§ÌÇπÎêú Ïñ¥ÌÖêÏÖò Í∞ÄÏ§ëÏπò Ï†ÅÏö©
        masked_output = x * masked_attention_weights.unsqueeze(-1)  # (batch_size, time_steps, feature_dim)

        return masked_output, masked_attention_weights

class SlidingWindowAP(nn.Module):
    def __init__(self, window_size=5):
        super(SlidingWindowAP, self).__init__()
        self.window_size = window_size
        self.padding = window_size // 2
        self.avg_pool = nn.AvgPool1d(kernel_size=window_size, stride=1, padding=self.padding)

    def forward(self, x):
        # input x : batch, timestep, feature dim
        x = x.transpose(1,2)
        x = self.avg_pool(x)
        x = x.transpose(1, 2)

        return x

class LayerWiseBaseSlidingPool(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseBaseSlidingPool, self).__init__()
        self.mask_ratio = mask_ratio  # Proportion of frames to mask
        self.num_layers = num_layers  # Number of layers to handle
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # Learnable weights for each layer
        self.aspm_modules = nn.ModuleList([ASPMSoftmax(input_dim=input_dim) for _ in range(num_layers)])
        self.sliding_pool = SlidingWindowAP(window_size=5)

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor): Tensor of shape [Batch, Layer, Frame, Dim],
                                       containing the embeddings for each layer.
        Returns:
            torch.Tensor: Utterance-level embedding of shape [Batch, Dim].
        """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []
        attn_weights = []

        for layer_idx in range(num_layers):
            # Get the embeddings for the current layer
            layer_output = layer_reps[:, layer_idx, :, :]  # Shape: [Batch, Frame, Dim]

            # Generate masked output for the current layer using its own ASPM module
            masked_output, attention_map = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)  # Shape: [Batch, Frame, Dim]
            sliding_pooled_output = self.sliding_pool(masked_output)

            # Append the masked output
            masked_outputs.append(sliding_pooled_output)
            attn_weights.append(attention_map)

        # self.get_attention_map(attn_weights)
        # Stack masked outputs and apply learnable weights
        masked_outputs = torch.stack(masked_outputs, dim=1)  # Shape: [Batch, Layer, Frame, Dim]
        weighted_outputs = (masked_outputs * self.layer_weights.view(1, -1, 1, 1)).sum(dim=1)  # Shape: [Batch, Frame, Dim]

        # Utterance-level average pooling
        utterance_level_embedding = torch.mean(weighted_outputs, dim=1)  # Shape: [Batch, Dim]

        return utterance_level_embedding

class LayerWiseASP(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.5):
        super(LayerWiseASP, self).__init__()
        self.mask_ratio = mask_ratio  # Proportion of frames to mask
        self.num_layers = num_layers  # Number of layers to handle
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # Learnable weights for each layer
        self.aspm_modules = nn.ModuleList([ASPM(input_dim=input_dim) for _ in range(num_layers)])
        self.final_attention_vector = nn.Linear(input_dim, 1, bias=True)  # v_t for utterance-level pooling
        self.final_linear = nn.Linear(input_dim, input_dim, bias=True)  # U_t for utterance-level pooling

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor): Tensor of shape [Batch, Layer, Frame, Dim],
                                       containing the embeddings for each layer.
        Returns:
            torch.Tensor: Utterance-level embedding of shape [Batch, Dim].
        """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []

        for layer_idx in range(num_layers):
            # Get the embeddings for the current layer
            layer_output = layer_reps[:, layer_idx, :, :]  # Shape: [Batch, Frame, Dim]

            # Generate masked output for the current layer using its own ASPM module
            masked_output = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)  # Shape: [Batch, Frame, Dim]

            # Append the masked output
            masked_outputs.append(masked_output)

        # Stack masked outputs and apply learnable weights
        masked_outputs = torch.stack(masked_outputs, dim=1)  # Shape: [Batch, Layer, Frame, Dim]
        weighted_outputs = (masked_outputs * self.layer_weights.view(1, -1, 1, 1)).sum(dim=1)  # Shape: [Batch, Frame, Dim]

        # Utterance-level attention pooling
        scores = self.final_linear(weighted_outputs)  # Shape: [Batch, Frame, Dim]
        scores = torch.tanh(scores)  # Apply tanh activation
        scores = self.final_attention_vector(scores).squeeze(-1)  # Shape: [Batch, Frame]

        attention_weights = F.softmax(scores, dim=1)  # Shape: [Batch, Frame]
        utterance_level_embedding = torch.sum(weighted_outputs * attention_weights.unsqueeze(-1), dim=1)  # Shape: [Batch, Dim]

        return utterance_level_embedding

class LayerWiseBasePooling(nn.Module):
    """
    - Î†àÏù¥Ïñ¥Î≥ÑÎ°ú Ïñ¥ÌÖêÌã∞Î∏å ÌíÄÎßÅ (ÎßàÏä§ÌÇπ ÏóÜÏùå) ÏàòÌñâ
    - Î†àÏù¥Ïñ¥Î≥Ñ ÌïôÏäµ Í∞ÄÏ§ëÏπò layer_weightsÎ°ú Ìï©ÏÇ∞
    - ÏµúÏ¢Ö (B, D) ÏûÑÎ≤†Îî© Ï∂úÎ†•
    """
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseBasePooling, self).__init__()
        self.num_layers = num_layers
        self.mask_ratio = mask_ratio

        # Î†àÏù¥Ïñ¥Î≥Ñ ÌïôÏäµ Í∞ÄÏ§ëÏπò
        self.layer_weights = nn.Parameter(torch.ones(num_layers))

        # Í∞Å Î†àÏù¥Ïñ¥Ïóê ÎåÄÏùëÌïòÎäî AttentivePooling Î™®Îìà
        self.attentive_poolers = nn.ModuleList(
            [ASP(input_dim) for _ in range(num_layers)]
        )

    def forward(self, layer_reps):
        """
        Args:
            layer_reps: (Batch, Layer, Frame, Dim)
        Returns:
            utterance_level_embedding: (Batch, Dim)
        """
        # ÎßåÏïΩ list ÌòïÌÉúÎùºÎ©¥ tensorÎ°ú Î≥ÄÌôò
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)
        # shape: (B, num_layers, F, D)

        batch_size, num_layers, frame_size, dim = layer_reps.shape

        pooled_outputs = []     # Î†àÏù¥Ïñ¥Î≥Ñ (B, D)
        attention_maps = []     # Î†àÏù¥Ïñ¥Î≥Ñ (B, F)

        # 1) Í∞Å Î†àÏù¥Ïñ¥Î≥Ñ Ïñ¥ÌÖêÌã∞Î∏å ÌíÄÎßÅ
        for layer_idx in range(num_layers):
            x_layer = layer_reps[:, layer_idx, :, :]  # (B, F, D)
            pooled, attn_map = self.attentive_poolers[layer_idx](x_layer)
            # pooled: (B, D)
            # attn_map: (B, F)

            pooled_outputs.append(pooled)
            attention_maps.append(attn_map)

        # 2) Î†àÏù¥Ïñ¥ ÌíÄÎßÅ Í≤∞Í≥º Ïä§ÌÉù: (B, num_layers, D)
        pooled_outputs = torch.stack(pooled_outputs, dim=1)

        # 3) Î†àÏù¥Ïñ¥ Í∞ÄÏ§ëÏπò Í≥± ‚Üí Ìï©ÏÇ∞
        #    layer_weights: (num_layers,) ‚Üí (1, num_layers, 1)
        #    => Î∏åÎ°úÎìúÏ∫êÏä§ÌåÖ Í≥± ÌõÑ sum(dim=1) => (B, D)
        weighted_sum = (
            pooled_outputs
            * self.layer_weights.view(1, -1, 1)
        ).sum(dim=1)

        # (B, D)
        utterance_level_embedding = weighted_sum

        return utterance_level_embedding

class LayerWiseWeightScale(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0, scale_factor_init=1.0, scale_factor_min=0.1, scale_factor_max=10.0):
        super(LayerWiseWeightScale, self).__init__()
        self.mask_ratio = mask_ratio  # Proportion of frames to mask
        self.num_layers = num_layers  # Number of layers to handle

        # (1) ÌïôÏäµ Í∞ÄÎä•Ìïú Î†àÏù¥Ïñ¥ Í∞ÄÏ§ëÏπò (Softmax Ï†ïÍ∑úÌôî ÎåÄÏÉÅ)
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # Ï¥àÍ∏∞Í∞í 1

        # (2) ÌïôÏäµ Í∞ÄÎä•Ìïú Scale Factor (Ï¥àÍ∏∞Í∞í ÏÑ§Ï†ï Î∞è ÌÅ¥Î¶¨Ìïë Î≤îÏúÑ ÏßÄÏ†ï)
        self.scale_factor = nn.Parameter(torch.tensor(scale_factor_init))  # ÌïôÏäµ Í∞ÄÎä•Ìïú Scale Factor Ï¥àÍ∏∞Í∞í
        self.scale_factor_min = scale_factor_min
        self.scale_factor_max = scale_factor_max

        # (3) Î†àÏù¥Ïñ¥Î≥Ñ ASPM Î™®Îìà (ÎûúÎç§ ÎßàÏä§ÌÇπ ÏÇ¨Ïö© ÏòàÏãú)
        self.aspm_modules = nn.ModuleList([ASPMSoftmax(input_dim=input_dim) for _ in range(num_layers)])

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor): Tensor of shape [Batch, Layer, Frame, Dim],
                                       containing the embeddings for each layer.
        Returns:
            torch.Tensor: Utterance-level embedding of shape [Batch, Dim].
        """
        # 0) ÏûÖÎ†• ÌòïÌÉúÍ∞Ä listÎùºÎ©¥ [Batch, Layer, Frame, Dim]ÏúºÎ°ú Î≥ÄÌôò
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []
        attn_weights = []

        # 1) Î†àÏù¥Ïñ¥Î≥Ñ ASPM Î™®ÎìàÏùÑ ÌÜµÌï¥ ÎßàÏä§ÌÇπÎêú Ï∂úÎ†•Í≥º Ïñ¥ÌÖêÏÖò Îßµ Í≥ÑÏÇ∞
        for layer_idx in range(num_layers):
            layer_output = layer_reps[:, layer_idx, :, :]  # [Batch, Frame, Dim]
            masked_output, attention_map = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)
            masked_outputs.append(masked_output)
            attn_weights.append(attention_map)

        # 2) ÎßàÏä§ÌÇπÎêú Ï∂úÎ†•Îì§ÏùÑ [Batch, Layer, Frame, Dim] ÌòïÌÉúÎ°ú Ïä§ÌÉù
        masked_outputs = torch.stack(masked_outputs, dim=1)

        # 3) ÌïôÏäµ Í∞ÄÎä•Ìïú Scale FactorÎ•º Í≥±Ìïú Softmax Ï†ïÍ∑úÌôîÎêú Î†àÏù¥Ïñ¥ Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
        normalized_layer_weights = F.softmax(self.layer_weights, dim=0) * self.scale_factor

        # 4) Î†àÏù¥Ïñ¥Î≥Ñ Í∞ÄÏ§ëÌï©ÏùÑ Í≥ÑÏÇ∞ (Weighted Sum)
        weighted_outputs = (masked_outputs * normalized_layer_weights.view(1, -1, 1, 1)).sum(dim=1)  # [Batch, Frame, Dim]

        # 5) Utterance-level ÌèâÍ∑† ÌíÄÎßÅ (Average Pooling)
        utterance_level_embedding = torch.mean(weighted_outputs, dim=1)  # [Batch, Dim]

        # 6) Gradient ClippingÏùÑ ÌÜµÌï¥ ÌïôÏäµ Í∞ÄÎä•Ìïú Scale FactorÏùò Í∞í Ï†úÌïú
        with torch.no_grad():
            self.scale_factor.data = torch.clamp(self.scale_factor, min=self.scale_factor_min, max=self.scale_factor_max)

        return utterance_level_embedding

class LayerWiseBase(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseBase, self).__init__()
        self.mask_ratio = mask_ratio  # Proportion of frames to mask
        self.num_layers = num_layers  # Number of layers to handle
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # Learnable weights for each layer
        self.aspm_modules = nn.ModuleList([ASPMSoftmax(input_dim=input_dim) for _ in range(num_layers)])

    def forward(self, layer_reps):
        """s
        Args:
            layer_reps (torch.Tensor): Tensor of shape [Batch, Layer, Frame, Dim],
                                       containing the embeddings for each layer.
        Returns:
            torch.Tensor: Utterance-level embedding of shape [Batch, Dim].
        """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []
        attn_weights = []

        for layer_idx in range(num_layers):
            # Get the embeddings for the current layer
            layer_output = layer_reps[:, layer_idx, :, :]  # Shape: [Batch, Frame, Dim]

            # Generate masked output for the current layer using its own ASPM module
            masked_output, attention_map = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)  # Shape: [Batch, Frame, Dim]

            # Append the masked output
            masked_outputs.append(masked_output)
            attn_weights.append(attention_map)

        # self.get_attention_map(attn_weights)
        # Stack masked outputs and apply learnable weights
        masked_outputs = torch.stack(masked_outputs, dim=1)  # Shape: [Batch, Layer, Frame, Dim]
        weighted_outputs = (masked_outputs * self.layer_weights.view(1, -1, 1, 1)).sum(dim=1)  # Shape: [Batch, Frame, Dim]

        # Utterance-level average pooling
        utterance_level_embedding = torch.mean(weighted_outputs, dim=1)  # Shape: [Batch, Dim]

        return utterance_level_embedding

    def get_attention_map(self, attention_maps, save_dir="./attention_maps_softmax"):
        """ 
        üîπ Ïñ¥ÌÖêÏÖò Îßµ (Heatmap) + Î†àÏù¥Ïñ¥Î≥Ñ 1D Í∑∏ÎûòÌîÑÎ•º ÌïòÎÇòÏùò ÌîºÍ∑úÏñ¥Î°ú Ï†ÄÏû•ÌïòÎäî Ìï®Ïàò
        Args:
            attention_maps: (Batch, Layers, Time Steps)
            save_dir: Ï†ÄÏû•Ìï† ÎîîÎ†âÌÜ†Î¶¨
        """
        os.makedirs(save_dir, exist_ok=True)  # üî• Ï†ÄÏû•Ìï† Ìè¥Îçî ÏÉùÏÑ±

        attention_maps = torch.stack(attention_maps, dim=1).detach()  # (Batch, Layers, Time Steps, 1)
        batch_size, num_layers, time_steps = attention_maps.shape

        for batch_idx in range(batch_size):  # üî• Î∞∞ÏπòÎ≥ÑÎ°ú Ï†ÄÏû•
            avg_attention = attention_maps[batch_idx].squeeze(-1)  # (Layers, Time Steps)

            fig, axes = plt.subplots(2, 1, figsize=(12, 10), gridspec_kw={'height_ratios': [3, 2]})  # üî• ÌûàÌä∏Îßµ: ÏÑ† Í∑∏ÎûòÌîÑ = 3:2 ÎπÑÏú®

            ## üîπ (1) Ïñ¥ÌÖêÏÖò Îßµ ÌûàÌä∏Îßµ (ÏúÑ)
            sns.heatmap(avg_attention.cpu().numpy(), cmap="Reds", ax=axes[0], square="auto")
            axes[0].set_xlabel("Time Steps", fontsize=10)
            axes[0].set_ylabel("Layers", fontsize=10)
            axes[0].set_title(f"Attention Map (Sample {batch_idx})", fontsize=12)

            ## üîπ (2) Ïñ¥ÌÖêÏÖò Í∞í 1D ÏÑ† Í∑∏ÎûòÌîÑ (ÏïÑÎûò)
            for layer_idx in range(num_layers):
                axes[1].plot(
                    range(time_steps), avg_attention[layer_idx].cpu().numpy(), label=f"Layer {layer_idx}"
                )

            axes[1].set_xlabel("Time Steps", fontsize=10)
            axes[1].set_ylabel("Attention Weight", fontsize=10)
            axes[1].set_title("Layer-wise Attention Distribution over Time", fontsize=12)
            axes[1].legend(loc="upper right", fontsize=8)
            axes[1].grid(True)

            ## üîπ Ï†ÄÏû•
            save_path = os.path.join(save_dir, f"attn_map_sample_{batch_idx}.png")
            plt.tight_layout()  # Î†àÏù¥ÏïÑÏõÉ Ï°∞Ï†ï
            plt.savefig(save_path, dpi=300)  # üî• Ï†ÄÏû•
            plt.close()

            print(f"‚úÖ Ïñ¥ÌÖêÏÖò Îßµ + 1D Í∑∏ÎûòÌîÑ Ï†ÄÏû• ÏôÑÎ£å: {save_path}")

class LayerWiseNone(nn.Module):
    def __init__(self, num_layers, mask_ratio=0.0):
        super(LayerWiseNone, self).__init__()
        self.mask_ratio = mask_ratio
        self.num_layers = num_layers
        # Î†àÏù¥Ïñ¥Î≥Ñ ÌïôÏäµ Í∞ÄÏ§ëÏπò (Ï¥àÍ∏∞Í∞í 1)
        self.layer_weights = nn.Parameter(torch.ones(num_layers))

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor or list):
                - [Batch, Layer, Frame, Dim] ÌòïÌÉúÏùò ÌÖêÏÑú
                  (Î†àÏù¥Ïñ¥ Ïàò = self.num_layers)
                - ÎßåÏïΩ list ÌòïÌÉúÎùºÎ©¥, [Batch, Layer, Frame, Dim]ÏúºÎ°ú stack Ï≤òÎ¶¨
        Returns:
            torch.Tensor: [Batch, Dim] ÌòïÌÉúÏùò ÏµúÏ¢Ö ÏûÑÎ≤†Îî©
        """
        # ÎßåÏïΩ layer_repsÍ∞Ä listÎ°ú Îì§Ïñ¥Ïò®Îã§Î©¥ tensorÎ°ú Î≥ÄÌôò
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)
        
        # layer_reps.shape: (Batch, num_layers, Frame, Dim)

        # 1) Î†àÏù¥Ïñ¥Î≥Ñ Í∞ÄÏ§ëÏπò Í≥± + Ìï©ÏÇ∞
        #    layer_weights (num_layers,) ‚Üí (1, num_layers, 1, 1)
        #    => Î†àÏù¥Ïñ¥ Ï∞®Ïõê(num_layers)Ïóê ÎåÄÌï¥ Î∏åÎ°úÎìúÏ∫êÏä§ÌåÖ Í≥±
        weighted_sum = (
            layer_reps * self.layer_weights.view(1, -1, 1, 1)
        ).sum(dim=1)  # Í≤∞Í≥º shape: [Batch, Frame, Dim]

        # 2) ÌîÑÎ†àÏûÑ Ï∞®ÏõêÏóê ÎåÄÌï¥ ÌèâÍ∑† ÌíÄÎßÅ -> [Batch, Dim]
        utterance_level_embedding = weighted_sum.mean(dim=1)

        return utterance_level_embedding

class LayerWiseNoneSelect(nn.Module):
    def __init__(self, num_layers, mask_ratio=0.0):
        super(LayerWiseNoneSelect, self).__init__()
        self.mask_ratio = mask_ratio
        self.num_layers = num_layers
        
        # Î†àÏù¥Ïñ¥Î≥Ñ ÌïôÏäµ Í∞ÄÏ§ëÏπò (Ï¥àÍ∏∞Í∞í 1)
        self.layer_weights = nn.Parameter(torch.ones(num_layers))

        # ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú Ïó¨Îü¨ Î†àÏù¥Ïñ¥ Ïù∏Îç±Ïä§ ÏßÄÏ†ï (0-based)
        # Ïòà: [5, 6, 7] ‚Üí ÏÇ¨Îûå Í∏∞Ï§Ä(1-based)ÏúºÎ°† 6,7,8Î≤àÏß∏ Î†àÏù¥Ïñ¥
        self.selected_layers = [5, 6, 7, 10, 12]

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor or list):
                - [Batch, Layer, Frame, Dim] ÌòïÌÉú (num_layers = self.num_layers)
                - listÎùºÎ©¥ stackÌïòÏó¨ Î™®Ïñë ÌÜµÏùº
        Returns:
            torch.Tensor: [Batch, Dim] ÌòïÌÉú ÏµúÏ¢Ö ÏûÑÎ≤†Îî©
        """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)
        # => (B, num_layers, F, D)

        # (1) Ïó¨Îü¨ Î†àÏù¥Ïñ¥Î•º ÏÑ†ÌÉùÌï¥ÏÑú Ï∂îÏ∂ú
        #     (B, selected_layer_count, F, D) ÌòïÌÉú
        selected_out = layer_reps[:, self.selected_layers, :, :]
        
        # (2) ÏÑ†ÌÉùÎêú Î†àÏù¥Ïñ¥Ïùò Í∞ÄÏ§ëÏπòÎßå Í∞ÄÏ†∏Ïò¥
        #     (num_layers,) ‚Üí (selected_layer_count,)
        selected_weights = self.layer_weights[self.selected_layers]

        # (3) Î†àÏù¥Ïñ¥Î≥Ñ Í∞ÄÏ§ëÏπò Í≥±
        #     selected_out.shape = (B, selected_layer_count, F, D)
        #     selected_weights.shape = (selected_layer_count,)
        #     Î∏åÎ°úÎìúÏ∫êÏä§ÌåÖ ÏúÑÌï¥ (1, selected_layer_count, 1, 1)
        weighted_sum = (
            selected_out * selected_weights.view(1, -1, 1, 1)
        ).sum(dim=1)  # (B, F, D)

        # (4) ÌîÑÎ†àÏûÑ Ï∞®Ïõê ÌèâÍ∑† ÌíÄÎßÅ ‚Üí (B, D)
        utterance_level_embedding = weighted_sum.mean(dim=1)

        return utterance_level_embedding

class LayerWiseNoneAttn(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseNoneAttn, self).__init__()
        self.mask_ratio = mask_ratio  # Ïó¨Í∏∞ÏÑúÎäî ÏÇ¨Ïö© Ïïà Ìï®
        self.num_layers = num_layers

        # (1) Î†àÏù¥Ïñ¥Î≥Ñ ÌïôÏäµ Í∞ÄÏ§ëÏπò (Ï¥àÍ∏∞Í∞í 1)
        self.layer_weights = nn.Parameter(torch.ones(num_layers))

        # (2) ÏµúÏ¢Ö ÌîÑÎ†àÏûÑ ÌíÄÎßÅÏùÑ ÏúÑÌïú Ïñ¥ÌÖêÏÖò ÌååÌä∏(Attentive Pooling)
        self.att_linear = nn.Linear(input_dim, input_dim, bias=True)    # U^T
        self.att_vector = nn.Linear(input_dim, 1, bias=True)           # v^T

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor or list):
                - [Batch, num_layers, Frame, Dim] ÌòïÌÉú ÌÖêÏÑú
                - listÎ°ú Îì§Ïñ¥Ïò§Î©¥ stackÌïòÏó¨ Î™®Ïñë ÌÜµÏùº
        Returns:
            utterance_level_embedding: [Batch, Dim] ÏµúÏ¢Ö ÏûÑÎ≤†Îî©
        """
        # 0) ÎßåÏïΩ listÎùºÎ©¥ [Batch, Layer, Frame, Dim]ÏúºÎ°ú Î≥ÄÌôò
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)
        # => shape: (B, num_layers, Frame, Dim)

        # 1) Î†àÏù¥Ïñ¥Î≥Ñ Í∞ÄÏ§ëÏπò Í≥± + Ìï©ÏÇ∞
        #    layer_weights: (num_layers,) ‚Üí (1, num_layers, 1, 1)Î°ú reshape
        #    Î†àÏù¥Ïñ¥ Ï∞®Ïõê(num_layers) Í∏∞Ï§ÄÏúºÎ°ú Ìï©ÏÇ∞ÌïòÎ©¥ -> (B, Frame, Dim)
        weighted_sum = (
            layer_reps * self.layer_weights.view(1, -1, 1, 1)
        ).sum(dim=1)  # shape: [Batch, Frame, Dim]

        # 2) ÎßàÏßÄÎßâ ÌîÑÎ†àÏûÑ ÌíÄÎßÅÏùÑ Ïñ¥ÌÖêÌã∞Î∏å ÌíÄÎßÅÏúºÎ°ú ÏàòÌñâ
        #    (B, Frame, Dim) ‚Üí (B, Dim)
        # 2-1) Ï†êÏàò Í≥ÑÏÇ∞: Linear ‚Üí tanh ‚Üí Linear
        scores = self.att_linear(weighted_sum)         # (B, F, D)
        scores = torch.tanh(scores)
        scores = self.att_vector(scores).squeeze(-1)   # (B, F)

        # 2-2) SoftmaxÎ°ú ÌîÑÎ†àÏûÑÎ≥Ñ Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
        attention_weights = F.softmax(scores, dim=1)    # (B, F)

        # 2-3) Í∞ÄÏ§ëÌï©(Weighted Sum)
        #      (B, F, D) * (B, F) -> (B, D)
        utterance_level_embedding = torch.sum(
            weighted_sum * attention_weights.unsqueeze(-1), 
            dim=1
        )
        # self.save_attention_map(attention_weights)

        return utterance_level_embedding

    def save_attention_map(self, attention_weights):
        """
        üîπ Îã®Ïùº Ïñ¥ÌÖêÏÖò ÎßµÏùÑ ÏãúÍ∞ÅÌôîÌïòÍ≥† Ï†ÄÏû•ÌïòÎäî Ìï®Ïàò
        Args:
            attention_weights: (Batch, Time Steps)
        """
        save_dir = "after_attn_map"
        os.makedirs(save_dir, exist_ok=True)  # üî• Ï†ÄÏû•Ìï† Ìè¥Îçî ÏÉùÏÑ±

        batch_size, time_steps = attention_weights.shape

        for batch_idx in range(batch_size):
            attn_map = attention_weights[batch_idx].cpu().detach().numpy()  # (Time Steps,)

            fig, axes = plt.subplots(2, 1, figsize=(10, 8), gridspec_kw={'height_ratios': [3, 2]})

            ## üîπ (1) Ïñ¥ÌÖêÏÖò Îßµ ÌûàÌä∏Îßµ (ÏúÑ)
            sns.heatmap(attn_map[None, :], cmap="Reds", ax=axes[0], cbar=True, xticklabels=50, yticklabels=[])
            axes[0].set_xlabel("Time Steps", fontsize=10)
            axes[0].set_title(f"Attention Map (Sample {batch_idx})", fontsize=12)

            ## üîπ (2) Ïñ¥ÌÖêÏÖò Í∞í 1D ÏÑ† Í∑∏ÎûòÌîÑ (ÏïÑÎûò)
            axes[1].plot(range(time_steps), attn_map, label=f"Attention Weight", color='red')
            axes[1].set_xlabel("Time Steps", fontsize=10)
            axes[1].set_ylabel("Attention Weight", fontsize=10)
            axes[1].set_title("Frame-wise Attention Distribution", fontsize=12)
            axes[1].grid(True)

            ## üîπ Ï†ÄÏû•
            save_path = os.path.join(save_dir, f"attn_map_sample_{batch_idx}.png")
            plt.tight_layout()  # Î†àÏù¥ÏïÑÏõÉ Ï°∞Ï†ï
            plt.savefig(save_path, dpi=300)  # üî• Ï†ÄÏû•
            plt.close()

            print(f"‚úÖ Ïñ¥ÌÖêÏÖò Îßµ Ï†ÄÏû• ÏôÑÎ£å: {save_path}")

class LayerWiseSelect(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseSelect, self).__init__()
        self.mask_ratio = mask_ratio
        self.num_layers = num_layers
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  
        self.aspm_modules = nn.ModuleList(
            [ASPM(input_dim=input_dim) for _ in range(num_layers)]
        )

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor): [Batch, Layer, Frame, Dim]
        Returns:
            torch.Tensor: [Batch, Dim] Î∞úÌôî(utterance) ÏûÑÎ≤†Îî©
        """
        # layer_repsÍ∞Ä listÎùºÎ©¥, [Batch, Layer, Frame, Dim] ÌòïÌÉúÎ°ú Ïä§ÌÉù
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        # Ïòà: ÌäπÏ†ï Î†àÏù¥Ïñ¥Îßå ÏÑ†ÌÉùÌï¥ÏÑú ÏÇ¨Ïö©
        # 5 6 7 10 12
        selected_layers = [5, 6, 7]
        # ‚Äª Ïã§Ï†úÎ°úÎäî 6, 7, 8 Î†àÏù¥Ïñ¥Í∞Ä Ï°¥Ïû¨ÌïòÎäîÏßÄ(num_layers > 8) Ï≤¥ÌÅ¨Í∞Ä ÌïÑÏöîÌï† Ïàò ÏûàÏùå.

        # ÏÑ†ÌÉùÎêú Î†àÏù¥Ïñ¥Ïùò masked_output/attention_map Ï†ÄÏû•
        selected_outputs = []
        selected_attn_weights = []

        for layer_idx in selected_layers:
            # Ìï¥Îãπ Î†àÏù¥Ïñ¥ ÏûÑÎ≤†Îî©
            layer_output = layer_reps[:, layer_idx, :, :]  # [Batch, Frame, Dim]

            # Î¨¥ÏûëÏúÑ ÎßàÏä§ÌÇπ ASPM Î™®Îìà Ï†ÅÏö©
            masked_output, attention_map = self.aspm_modules[layer_idx](
                layer_output, mask_ratio=self.mask_ratio
            )
            selected_outputs.append(masked_output)
            selected_attn_weights.append(attention_map)

        # [Batch, #selected_layers, Frame, Dim] ÌòïÌÉúÎ°ú Ìï©ÏπòÍ∏∞
        selected_outputs = torch.stack(selected_outputs, dim=1)

        # ÏÑ†ÌÉùÎêú Î†àÏù¥Ïñ¥Ïùò Í∞ÄÏ§ëÏπòÎßå Í≥®ÎùºÏò¥
        selected_layer_weights = self.layer_weights[selected_layers]  
        # [num_layers] ‚Üí [#selected_layers]

        # Î†àÏù¥Ïñ¥ Í∞ÄÏ§ëÏπò Í≥±Ìï¥Ï§Ä Îí§ sum(Î†àÏù¥Ïñ¥ Ï∞®Ïõê)
        weighted_outputs = (selected_outputs *
                            selected_layer_weights.view(1, -1, 1, 1)).sum(dim=1)
        # Í≤∞Í≥º: [Batch, Frame, Dim]
        # ÏµúÏ¢Ö ÌîÑÎ†àÏûÑ Ï∞®Ïõê ÌèâÍ∑† ‚Üí [Batch, Dim]
        utterance_level_embedding = torch.mean(weighted_outputs, dim=1)

        return utterance_level_embedding

class LayerWiseBaseEpoch(nn.Module):
    def __init__(self, input_dim, num_layers, total_epochs=15, mask_ratio=0.0):
        super(LayerWiseBaseEpoch, self).__init__()
        self.num_layers = num_layers
        self.total_epochs = total_epochs
        self.current_epoch = 0  # üî• ÌïôÏäµ ÏãúÏûë Ïãú epochÏùÑ 0ÏúºÎ°ú Ï¥àÍ∏∞Ìôî
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # ÌïôÏäµ Í∞ÄÎä•Ìïú Î†àÏù¥Ïñ¥ Í∞ÄÏ§ëÏπò
        self.aspm_modules = nn.ModuleList([ASPM(input_dim=input_dim) for _ in range(num_layers)])

        # Ï¥àÎ∞òÏóêÎäî ÎßàÏä§ÌÇπ ÏóÜÏùå (0%) ‚Üí ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú 50%ÍπåÏßÄ Ï¶ùÍ∞Ä
        self.initial_mask_ratio = 0.0  # üî• 0% (Ï¥àÎ∞ò 5 epoch ÎèôÏïà)
        self.increase_epochs = 10
        self.mask_ratio = mask_ratio      # üî• ÏµúÏ¢Ö 50%

    def update_epoch(self):
        """ üî• Ïô∏Î∂ÄÏóêÏÑú Ìò∏Ï∂úÌïòÏó¨ ÌòÑÏû¨ epoch Í∞íÏùÑ ÏóÖÎç∞Ïù¥Ìä∏ÌïòÎäî Ìï®Ïàò """
        self.current_epoch = min(self.current_epoch + 1, self.total_epochs)

    def get_mask_ratios(self):
        """ üî• Ï¥àÎ∞ò 10 epoch ÎèôÏïà 50%ÍπåÏßÄ Ï¶ùÍ∞Ä, Ïù¥ÌõÑ Í≥†Ï†ï """
        if self.current_epoch < self.increase_epochs:
            # üî• 10 epoch ÎèôÏïà ÏÑ†ÌòïÏ†ÅÏúºÎ°ú Ï¶ùÍ∞Ä
            progress = self.current_epoch / self.increase_epochs
            mask_ratio = self.initial_mask_ratio + (self.mask_ratio - self.initial_mask_ratio) * progress
        else:
            # üî• 10 epoch Ïù¥ÌõÑÏóêÎäî 50% Í≥†Ï†ï
            mask_ratio = self.mask_ratio
       
        return [mask_ratio] * self.num_layers  # Î™®Îì† Î†àÏù¥Ïñ¥Ïóê ÎèôÏùºÌïú ÎπÑÏú® Ï†ÅÏö©


    def forward(self, layer_reps):
        """ üî• forwardÏóêÏÑú epochÏùÑ Îî∞Î°ú ÏûÖÎ†•ÌïòÏßÄ ÏïäÏïÑÎèÑ Îê® """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []

        mask_ratios = self.get_mask_ratios()  # üî• ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú epochÏùÑ Ï∞∏Ï°∞ÌïòÏó¨ ÎßàÏä§ÌÇπ ÎπÑÏú® Í≥ÑÏÇ∞

        
        for layer_idx in range(num_layers):
            layer_output = layer_reps[:, layer_idx, :, :]
            mask_ratio = mask_ratios[layer_idx]

            masked_output = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)

            masked_outputs.append(masked_output)

        masked_outputs = torch.stack(masked_outputs, dim=1)

        weighted_sum = (masked_outputs * self.layer_weights.view(1, -1, 1, 1)).sum(dim=1)

        utterance_level_embedding = torch.mean(weighted_sum, dim=1)

        return utterance_level_embedding

class spk_extractor(nn.Module):
    def __init__(self, mask_ratio = 0.0, **kwargs):
        super(spk_extractor, self).__init__()

        print(f"üîπ [spk_extractor] Initialized with mask_ratio: {mask_ratio}")

        checkpoint = torch.load('baseline/pretrained/WavLM-Base+.pt')
        # print("Pre-trained Model: {}".format(kwargs['pretrained_model_path']))
        # checkpoint = torch.load(kwargs['pretrained_model_path'])
        cfg = WavLMConfig(checkpoint['cfg'])
        self.model = WavLM(cfg)
        self.loadParameters(checkpoint['model'])
        # Add LayerWiseASP module
        self.weighted_pooling = LayerWiseBase(input_dim=768, num_layers=13, mask_ratio=mask_ratio)
        self.final_linear = nn.Linear(768, 256, bias=True)  # Final projection layer


    def forward(self, wav_and_flag):
        
        x = wav_and_flag[0]

        cnn_outs, layer_results =  self.model.extract_features(x, output_layer=13)
        layer_reps = [x.transpose(0, 1) for x, _ in layer_results]
        # x = torch.stack(layer_reps).transpose(0,-1).transpose(0,1)

        utterance_level_embedding = self.weighted_pooling(layer_reps)

        final_output = self.final_linear(utterance_level_embedding) 

        # out = self.backend(x)
        return final_output

    def loadParameters(self, param):

        self_state = self.model.state_dict();
        loaded_state = param

        for name, param in loaded_state.items():
            origname = name;
            

            if name not in self_state:
                # print("%s is not in the model."%origname);
                continue;

            if self_state[name].size() != loaded_state[origname].size():
                print("Wrong parameter length: %s, model: %s, loaded: %s"%(origname, self_state[name].size(), loaded_state[origname].size()));
                continue;

            self_state[name].copy_(param);


def MainModel(**kwargs):
    mask_ratio = kwargs.get("mask_ratio", 0.0)
    print(f"üîπ [MainModel] Passing mask_ratio: {mask_ratio}")
    model = spk_extractor(**kwargs)
    return model

if __name__ == "__main__":
        
    from torchinfo import summary

    # Model initialization
    model = spk_extractor()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Dummy input for testing, shape (1, 48240)
    dummy_input = torch.randn(1, 40, 48240).to(device)

    # Correct input format for the forward method
    summary(model, input_data=(dummy_input))
