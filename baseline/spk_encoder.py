import math
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import LayerNorm
from baseline.WavLM import *

import torch
import torch.nn as nn

import matplotlib.pyplot as plt
import seaborn as sns

class ASPM(nn.Module):
    def __init__(self, input_dim):
        super(ASPM, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x, mask_ratio=0.5):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
            mask_ratio: Float, proportion of frames to mask (e.g., 0.3 for 30%)
        Returns:
            masked_output: Tensor of shape (batch_size, time_steps, feature_dim)
        """
        # Linear transformation U^T * h_t + p
        scores = self.linear(x)  # Shape: (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)  # Apply tanh activation

        # Attention score s_t = v^T * scores + q
        scores = self.attention_vector(scores).squeeze(-1)  # Shape: (batch_size, time_steps)

        # Sort scores to get indices of the lowest scores
        batch_size, time_steps = scores.shape
        num_mask_frames = int(time_steps * mask_ratio)

        sorted_scores, sorted_indices = torch.sort(scores, dim=1)
        mask_indices = sorted_indices[:, :num_mask_frames]  # Indices of the lowest scores

        # Create a mask tensor
        mask = torch.ones_like(scores, dtype=torch.float32)  # Start with all ones
        for i in range(batch_size):
            mask[i, mask_indices[i]] = 0  # Set the lowest-scoring frames to zero

        # Apply the mask: Set masked scores to a very small value (-inf)
        scores = torch.where(mask.bool(), scores, torch.full_like(scores, float('-inf')))

        # Softmax to compute attention weights Î±_t
        masked_attention_weights = F.softmax(scores, dim=1)  # Shape: (batch_size, time_steps)

        # ğŸ”¹ 0 ê°’ ê°œìˆ˜ ê³„ì‚° ë° ì¶œë ¥
        # ğŸ”¹ í–‰ë³„(í”„ë ˆì„ë³„)ë¡œ 0 ê°œìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ì¶œë ¥
        # zero_counts_per_row = (masked_attention_weights == 0).sum(dim=1)

        # for row_idx, zero_count in enumerate(zero_counts_per_row.tolist()):  # `.tolist()`ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
        #     print(f"ğŸ”¹ Speaker {row_idx}: {zero_count} zero values")
            
        # num_zeros = (masked_attention_weights == 0).sum().item()
        # print(f"ğŸ”¹ Masked Attention Weights - Zero Count: {num_zeros}")

        # Multiply attention weights with the input embeddings
        masked_output = x * masked_attention_weights.unsqueeze(-1)  # Shape: (batch_size, time_steps, feature_dim)

        return masked_output, masked_attention_weights

class ASPMSoftmax(nn.Module):
    def __init__(self, input_dim):
        super(ASPMSoftmax, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x, mask_ratio=0.7):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
            mask_ratio: Float, proportion of frames to mask (e.g., 0.3 for 30%)
        Returns:
            masked_output: Tensor of shape (batch_size, time_steps, feature_dim)
        """
        # Linear transformation U^T * h_t + p
        scores = self.linear(x)  # Shape: (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)  # Apply tanh activation

        # Attention score s_t = v^T * scores + q
        scores = self.attention_vector(scores).squeeze(-1)  # Shape: (batch_size, time_steps)

        # Softmax to compute attention weights Î±_t
        attention_weights = F.softmax(scores, dim=1)  # Shape: (batch_size, time_steps)

        # Sort scores to get indices of the lowest scores (for masking)
        batch_size, time_steps = scores.shape
        num_mask_frames = int(time_steps * mask_ratio)

        sorted_scores, sorted_indices = torch.sort(scores, dim=1)
        mask_indices = sorted_indices[:, :num_mask_frames]  # Indices of the lowest scores

        # Create a mask tensor (default: all ones)
        mask = torch.ones_like(attention_weights, dtype=torch.float32)  # Shape: (batch_size, time_steps)
        for i in range(batch_size):
            mask[i, mask_indices[i]] = 0  # Set the lowest-scoring frames to zero

        # Apply the mask: Set masked attention weights to zero
        masked_attention_weights = attention_weights * mask  # Shape: (batch_size, time_steps)

        # Normalize again to ensure sum of attention weights is valid (prevent division by zero)
        # masked_attention_weights = masked_attention_weights / masked_attention_weights.sum(dim=1, keepdim=True).clamp(min=1e-6)

        # Multiply attention weights with the input embeddings
        masked_output = x * masked_attention_weights.unsqueeze(-1)  # Shape: (batch_size, time_steps, feature_dim)

        return masked_output, masked_attention_weights

class ASPMSoftMasking(nn.Module):
    def __init__(self, input_dim):
        super(ASPMSoftMasking, self).__init__()
        self.input_dim = input_dim

        # Linear transformation layers for attention score calculation
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

        # Learnable vector H0 for soft masking
        self.H0 = nn.Parameter(torch.randn(input_dim))  # Learnable parameter for H0

    def forward(self, x, mask_ratio=0.7):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
        Returns:
            masked_output: Tensor of shape (batch_size, time_steps, feature_dim)
            masked_attention_weights: Tensor of shape (batch_size, time_steps)
        """
        batch_size, time_steps, feature_dim = x.shape

        # Linear transformation U^T * h_t + p
        scores = self.linear(x)  # Shape: (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)  # Apply tanh activation

        # Attention score s_t = v^T * scores + q
        scores = self.attention_vector(scores).squeeze(-1)  # Shape: (batch_size, time_steps)

        # Softmax to compute attention weights Î±_t
        attention_weights = F.softmax(scores, dim=1)  # Shape: (batch_size, time_steps)

        # Sort scores to get indices of the lowest scores (for masking)
        num_mask_frames = int(time_steps * mask_ratio)
        sorted_scores, sorted_indices = torch.sort(scores, dim=1)
        mask_indices = sorted_indices[:, :num_mask_frames]  # Indices of the lowest scores

        # Create a mask tensor (default: all ones)
        mask = torch.ones_like(attention_weights, dtype=torch.float32)  # Shape: (batch_size, time_steps)
        for i in range(batch_size):
            mask[i, mask_indices[i]] = 0  # Set the lowest-scoring frames to zero

        # Apply the mask to attention weights (Soft Masking)
        masked_attention_weights = attention_weights * mask  # Shape: (batch_size, time_steps)

        # Soft Masking: Apply learnable vector H0 when mask is 0
        H0_expanded = self.H0.unsqueeze(0).unsqueeze(0).expand(batch_size, time_steps, feature_dim)
        masked_output = x * masked_attention_weights.unsqueeze(-1) + (1 - masked_attention_weights.unsqueeze(-1)) * H0_expanded

        return masked_output, masked_attention_weights

class ASPMResidual(nn.Module):
    def __init__(self, input_dim):
        super(ASPMResidual, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x, mask_ratio=0.5):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
            mask_ratio: Float, proportion of frames to mask (e.g., 0.3 for 30%)
        Returns:
            residual_output: Tensor of shape (batch_size, time_steps, feature_dim)
        """
        # Linear transformation U^T * h_t + p
        scores = self.linear(x)  # Shape: (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)  # Apply tanh activation

        # Attention score s_t = v^T * scores + q
        scores = self.attention_vector(scores).squeeze(-1)  # Shape: (batch_size, time_steps)

        # Softmax to compute attention weights Î±_t
        attention_weights = F.softmax(scores, dim=1)  # Shape: (batch_size, time_steps)

        # Sort scores to get indices of the lowest scores (for masking)
        batch_size, time_steps = scores.shape
        num_mask_frames = int(time_steps * mask_ratio)

        sorted_scores, sorted_indices = torch.sort(scores, dim=1)
        mask_indices = sorted_indices[:, :num_mask_frames]  # Indices of the lowest scores

        # Create a mask tensor (default: all ones)
        mask = torch.ones_like(attention_weights, dtype=torch.float32)  # Shape: (batch_size, time_steps)
        for i in range(batch_size):
            mask[i, mask_indices[i]] = 0  # Set the lowest-scoring frames to zero

        # Apply the mask: Set masked attention weights to zero
        masked_attention_weights = attention_weights * mask  # Shape: (batch_size, time_steps)

        # Residual ë°©ì‹ ì ìš©: ê¸°ë³¸ ì •ë³´ ìœ ì§€í•˜ë©´ì„œ ê°€ì¤‘ì¹˜ ë°˜ì˜
        residual_output = x + masked_attention_weights.unsqueeze(-1) * x  # Shape: (batch_size, time_steps, feature_dim)

        return residual_output, masked_attention_weights

class ASP(nn.Module):
    def __init__(self, input_dim):
        super(ASP, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x):
        """
        Args:
            x: (batch_size, time_steps, feature_dim)
        Returns:
            pooled_output: (batch_size, feature_dim)
            attention_weights: (batch_size, time_steps)
        """
        # 1) ì„ í˜• ë³€í™˜ ë° tanh
        scores = self.linear(x)               # (B, T, D)
        scores = torch.tanh(scores)           # (B, T, D)

        # 2) ìŠ¤ì¹¼ë¼ ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° (v^T)
        scores = self.attention_vector(scores).squeeze(-1)  # (B, T)

        # 3) Softmax ë¡œ í”„ë ˆì„ë³„ ê°€ì¤‘ì¹˜ ì‚°ì¶œ
        attention_weights = F.softmax(scores, dim=1)        # (B, T)

        # 4) ê°€ì¤‘í•© (Weighted Sum) â†’ (B, D)
        pooled_output = torch.sum(
            x * attention_weights.unsqueeze(-1), 
            dim=1
        )

        return pooled_output, attention_weights

class ASPMRandom(nn.Module):
    def __init__(self, input_dim):
        super(ASPMRandom, self).__init__()
        self.linear = nn.Linear(input_dim, input_dim, bias=True)  # U^T
        self.attention_vector = nn.Linear(input_dim, 1, bias=True)  # v^T

    def forward(self, x, mask_ratio=0.5):
        """
        Args:
            x: Tensor of shape (batch_size, time_steps, feature_dim)
            mask_ratio: Float, ì „ì²´ í”„ë ˆì„ ì¤‘ ë§ˆìŠ¤í‚¹í•  ë¹„ìœ¨ (ì˜ˆ: 0.3ì´ë©´ 30%ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹)
        Returns:
            masked_output: Tensor of shape (batch_size, time_steps, feature_dim)
            masked_attention_weights: Tensor of shape (batch_size, time_steps)
        """
        # 1) ì„ í˜• ë³€í™˜ê³¼ tanh ì ìš©
        scores = self.linear(x)                    # (batch_size, time_steps, feature_dim)
        scores = torch.tanh(scores)                # tanh í™œì„±í™”
        scores = self.attention_vector(scores).squeeze(-1)  # (batch_size, time_steps)

        # 2) ë¬´ì‘ìœ„ ë§ˆìŠ¤í‚¹í•  ì¸ë±ìŠ¤ ì„ íƒ
        batch_size, time_steps = scores.shape
        num_mask_frames = int(time_steps * mask_ratio)  # ë§ˆìŠ¤í‚¹í•  í”„ë ˆì„ ìˆ˜

        # ëª¨ë“  ìœ„ì¹˜ë¥¼ 1ë¡œ ì´ˆê¸°í™”í•œ mask í…ì„œ
        mask = torch.ones_like(scores, dtype=torch.float32)  # (batch_size, time_steps)

        for i in range(batch_size):
            # randpermì„ ì´ìš©í•˜ì—¬ ë¬´ì‘ìœ„ í”„ë ˆì„ ì¸ë±ìŠ¤ ì¶”ì¶œ
            rand_indices = torch.randperm(time_steps)[:num_mask_frames]
            mask[i, rand_indices] = 0  # ë§ˆìŠ¤í‚¹í•  ìœ„ì¹˜ì— 0ì„ ì„¤ì •

        # 3) ì†Œí”„íŠ¸ë§¥ìŠ¤ â†’ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        attention_weights = F.softmax(scores, dim=1)  # (batch_size, time_steps)

        # 4) ë§ˆìŠ¤í‚¹ì´ 0ì¸ ê³³ì— ì–´í…ì…˜ ê°€ì¤‘ì¹˜ 0ìœ¼ë¡œ ë®ì–´ì”Œìš°ê¸°
        masked_attention_weights = attention_weights * mask  # (batch_size, time_steps)

        # 5) ì…ë ¥ xì— ë§ˆìŠ¤í‚¹ëœ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì ìš©
        masked_output = x * masked_attention_weights.unsqueeze(-1)  # (batch_size, time_steps, feature_dim)

        return masked_output, masked_attention_weights

class SlidingWindowAP(nn.Module):
    def __init__(self, window_size=5):
        super(SlidingWindowAP, self).__init__()
        self.window_size = window_size
        self.padding = window_size // 2
        self.avg_pool = nn.AvgPool1d(kernel_size=window_size, stride=1, padding=self.padding)

    def forward(self, x):
        # input x : batch, timestep, feature dim
        x = x.transpose(1,2)
        x = self.avg_pool(x)
        x = x.transpose(1, 2)

        return x

class LayerWiseBaseSlidingPool(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseBaseSlidingPool, self).__init__()
        self.mask_ratio = mask_ratio  # Proportion of frames to mask
        self.num_layers = num_layers  # Number of layers to handle
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # Learnable weights for each layer
        self.aspm_modules = nn.ModuleList([ASPMSoftmax(input_dim=input_dim) for _ in range(num_layers)])
        self.sliding_pool = SlidingWindowAP(window_size=5)

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor): Tensor of shape [Batch, Layer, Frame, Dim],
                                       containing the embeddings for each layer.
        Returns:
            torch.Tensor: Utterance-level embedding of shape [Batch, Dim].
        """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []
        attn_weights = []

        for layer_idx in range(num_layers):
            # Get the embeddings for the current layer
            layer_output = layer_reps[:, layer_idx, :, :]  # Shape: [Batch, Frame, Dim]

            # Generate masked output for the current layer using its own ASPM module
            masked_output, attention_map = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)  # Shape: [Batch, Frame, Dim]
            sliding_pooled_output = self.sliding_pool(masked_output)

            # Append the masked output
            masked_outputs.append(sliding_pooled_output)
            attn_weights.append(attention_map)

        # self.get_attention_map(attn_weights)
        # Stack masked outputs and apply learnable weights
        masked_outputs = torch.stack(masked_outputs, dim=1)  # Shape: [Batch, Layer, Frame, Dim]
        weighted_outputs = (masked_outputs * self.layer_weights.view(1, -1, 1, 1)).sum(dim=1)  # Shape: [Batch, Frame, Dim]

        # Utterance-level average pooling
        utterance_level_embedding = torch.mean(weighted_outputs, dim=1)  # Shape: [Batch, Dim]

        return utterance_level_embedding

class LayerWiseASP(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.5):
        super(LayerWiseASP, self).__init__()
        self.mask_ratio = mask_ratio  # Proportion of frames to mask
        self.num_layers = num_layers  # Number of layers to handle
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # Learnable weights for each layer
        self.aspm_modules = nn.ModuleList([ASPM(input_dim=input_dim) for _ in range(num_layers)])
        self.final_attention_vector = nn.Linear(input_dim, 1, bias=True)  # v_t for utterance-level pooling
        self.final_linear = nn.Linear(input_dim, input_dim, bias=True)  # U_t for utterance-level pooling

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor): Tensor of shape [Batch, Layer, Frame, Dim],
                                       containing the embeddings for each layer.
        Returns:
            torch.Tensor: Utterance-level embedding of shape [Batch, Dim].
        """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []

        for layer_idx in range(num_layers):
            # Get the embeddings for the current layer
            layer_output = layer_reps[:, layer_idx, :, :]  # Shape: [Batch, Frame, Dim]

            # Generate masked output for the current layer using its own ASPM module
            masked_output = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)  # Shape: [Batch, Frame, Dim]

            # Append the masked output
            masked_outputs.append(masked_output)

        # Stack masked outputs and apply learnable weights
        masked_outputs = torch.stack(masked_outputs, dim=1)  # Shape: [Batch, Layer, Frame, Dim]
        weighted_outputs = (masked_outputs * self.layer_weights.view(1, -1, 1, 1)).sum(dim=1)  # Shape: [Batch, Frame, Dim]

        # Utterance-level attention pooling
        scores = self.final_linear(weighted_outputs)  # Shape: [Batch, Frame, Dim]
        scores = torch.tanh(scores)  # Apply tanh activation
        scores = self.final_attention_vector(scores).squeeze(-1)  # Shape: [Batch, Frame]

        attention_weights = F.softmax(scores, dim=1)  # Shape: [Batch, Frame]
        utterance_level_embedding = torch.sum(weighted_outputs * attention_weights.unsqueeze(-1), dim=1)  # Shape: [Batch, Dim]

        return utterance_level_embedding

class LayerWiseBasePooling(nn.Module):
    """
    - ë ˆì´ì–´ë³„ë¡œ ì–´í…í‹°ë¸Œ í’€ë§ (ë§ˆìŠ¤í‚¹ ì—†ìŒ) ìˆ˜í–‰
    - ë ˆì´ì–´ë³„ í•™ìŠµ ê°€ì¤‘ì¹˜ layer_weightsë¡œ í•©ì‚°
    - ìµœì¢… (B, D) ì„ë² ë”© ì¶œë ¥
    """
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseBasePooling, self).__init__()
        self.num_layers = num_layers
        self.mask_ratio = mask_ratio

        # ë ˆì´ì–´ë³„ í•™ìŠµ ê°€ì¤‘ì¹˜
        self.layer_weights = nn.Parameter(torch.ones(num_layers))

        # ê° ë ˆì´ì–´ì— ëŒ€ì‘í•˜ëŠ” AttentivePooling ëª¨ë“ˆ
        self.attentive_poolers = nn.ModuleList(
            [ASP(input_dim) for _ in range(num_layers)]
        )

    def forward(self, layer_reps):
        """
        Args:
            layer_reps: (Batch, Layer, Frame, Dim)
        Returns:
            utterance_level_embedding: (Batch, Dim)
        """
        # ë§Œì•½ list í˜•íƒœë¼ë©´ tensorë¡œ ë³€í™˜
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)
        # shape: (B, num_layers, F, D)

        batch_size, num_layers, frame_size, dim = layer_reps.shape

        pooled_outputs = []     # ë ˆì´ì–´ë³„ (B, D)
        attention_maps = []     # ë ˆì´ì–´ë³„ (B, F)

        # 1) ê° ë ˆì´ì–´ë³„ ì–´í…í‹°ë¸Œ í’€ë§
        for layer_idx in range(num_layers):
            x_layer = layer_reps[:, layer_idx, :, :]  # (B, F, D)
            pooled, attn_map = self.attentive_poolers[layer_idx](x_layer)
            # pooled: (B, D)
            # attn_map: (B, F)

            pooled_outputs.append(pooled)
            attention_maps.append(attn_map)

        # 2) ë ˆì´ì–´ í’€ë§ ê²°ê³¼ ìŠ¤íƒ: (B, num_layers, D)
        pooled_outputs = torch.stack(pooled_outputs, dim=1)

        # 3) ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ê³± â†’ í•©ì‚°
        #    layer_weights: (num_layers,) â†’ (1, num_layers, 1)
        #    => ë¸Œë¡œë“œìºìŠ¤íŒ… ê³± í›„ sum(dim=1) => (B, D)
        weighted_sum = (
            pooled_outputs
            * self.layer_weights.view(1, -1, 1)
        ).sum(dim=1)

        # (B, D)
        utterance_level_embedding = weighted_sum

        return utterance_level_embedding

class LayerWiseWeightScale(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0, scale_factor_init=1.0, scale_factor_min=0.1, scale_factor_max=10.0):
        super(LayerWiseWeightScale, self).__init__()
        self.mask_ratio = mask_ratio  # Proportion of frames to mask
        self.num_layers = num_layers  # Number of layers to handle

        # (1) í•™ìŠµ ê°€ëŠ¥í•œ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ (Softmax ì •ê·œí™” ëŒ€ìƒ)
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # ì´ˆê¸°ê°’ 1

        # (2) í•™ìŠµ ê°€ëŠ¥í•œ Scale Factor (ì´ˆê¸°ê°’ ì„¤ì • ë° í´ë¦¬í•‘ ë²”ìœ„ ì§€ì •)
        self.scale_factor = nn.Parameter(torch.tensor(scale_factor_init))  # í•™ìŠµ ê°€ëŠ¥í•œ Scale Factor ì´ˆê¸°ê°’
        self.scale_factor_min = scale_factor_min
        self.scale_factor_max = scale_factor_max

        # (3) ë ˆì´ì–´ë³„ ASPM ëª¨ë“ˆ (ëœë¤ ë§ˆìŠ¤í‚¹ ì‚¬ìš© ì˜ˆì‹œ)
        self.aspm_modules = nn.ModuleList([ASPMSoftmax(input_dim=input_dim) for _ in range(num_layers)])

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor): Tensor of shape [Batch, Layer, Frame, Dim],
                                       containing the embeddings for each layer.
        Returns:
            torch.Tensor: Utterance-level embedding of shape [Batch, Dim].
        """
        # 0) ì…ë ¥ í˜•íƒœê°€ listë¼ë©´ [Batch, Layer, Frame, Dim]ìœ¼ë¡œ ë³€í™˜
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []
        attn_weights = []

        # 1) ë ˆì´ì–´ë³„ ASPM ëª¨ë“ˆì„ í†µí•´ ë§ˆìŠ¤í‚¹ëœ ì¶œë ¥ê³¼ ì–´í…ì…˜ ë§µ ê³„ì‚°
        for layer_idx in range(num_layers):
            layer_output = layer_reps[:, layer_idx, :, :]  # [Batch, Frame, Dim]
            masked_output, attention_map = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)
            masked_outputs.append(masked_output)
            attn_weights.append(attention_map)

        # 2) ë§ˆìŠ¤í‚¹ëœ ì¶œë ¥ë“¤ì„ [Batch, Layer, Frame, Dim] í˜•íƒœë¡œ ìŠ¤íƒ
        masked_outputs = torch.stack(masked_outputs, dim=1)

        # 3) í•™ìŠµ ê°€ëŠ¥í•œ Scale Factorë¥¼ ê³±í•œ Softmax ì •ê·œí™”ëœ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ê³„ì‚°
        normalized_layer_weights = F.softmax(self.layer_weights, dim=0) * self.scale_factor

        # 4) ë ˆì´ì–´ë³„ ê°€ì¤‘í•©ì„ ê³„ì‚° (Weighted Sum)
        weighted_outputs = (masked_outputs * normalized_layer_weights.view(1, -1, 1, 1)).sum(dim=1)  # [Batch, Frame, Dim]

        # 5) Utterance-level í‰ê·  í’€ë§ (Average Pooling)
        utterance_level_embedding = torch.mean(weighted_outputs, dim=1)  # [Batch, Dim]

        # 6) Gradient Clippingì„ í†µí•´ í•™ìŠµ ê°€ëŠ¥í•œ Scale Factorì˜ ê°’ ì œí•œ
        with torch.no_grad():
            self.scale_factor.data = torch.clamp(self.scale_factor, min=self.scale_factor_min, max=self.scale_factor_max)

        return utterance_level_embedding

class LayerWiseBase(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseBase, self).__init__()
        self.mask_ratio = mask_ratio  # Proportion of frames to mask
        self.num_layers = num_layers  # Number of layers to handle
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # Learnable weights for each layer
        self.aspm_modules = nn.ModuleList([ASPMSoftmax(input_dim=input_dim) for _ in range(num_layers)])

    def forward(self, layer_reps):
        """s
        Args:
            layer_reps (torch.Tensor): Tensor of shape [Batch, Layer, Frame, Dim],
                                       containing the embeddings for each layer.
        Returns:
            torch.Tensor: Utterance-level embedding of shape [Batch, Dim].
        """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []
        attn_weights = []

        for layer_idx in range(num_layers):
            # Get the embeddings for the current layer
            layer_output = layer_reps[:, layer_idx, :, :]  # Shape: [Batch, Frame, Dim]

            # Generate masked output for the current layer using its own ASPM module
            masked_output, attention_map = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)  # Shape: [Batch, Frame, Dim]

            # Append the masked output
            masked_outputs.append(masked_output)
            attn_weights.append(attention_map)

        # self.get_attention_map(attn_weights)
        # Stack masked outputs and apply learnable weights
        masked_outputs = torch.stack(masked_outputs, dim=1)  # Shape: [Batch, Layer, Frame, Dim]
        weighted_outputs = (masked_outputs * self.layer_weights.view(1, -1, 1, 1)).sum(dim=1)  # Shape: [Batch, Frame, Dim]

        # Utterance-level average pooling
        utterance_level_embedding = torch.mean(weighted_outputs, dim=1)  # Shape: [Batch, Dim]

        return utterance_level_embedding

    def get_attention_map(self, attention_maps, save_dir="./attention_maps_softmax"):
        """ 
        ğŸ”¹ ì–´í…ì…˜ ë§µ (Heatmap) + ë ˆì´ì–´ë³„ 1D ê·¸ë˜í”„ë¥¼ í•˜ë‚˜ì˜ í”¼ê·œì–´ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
        Args:
            attention_maps: (Batch, Layers, Time Steps)
            save_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        os.makedirs(save_dir, exist_ok=True)  # ğŸ”¥ ì €ì¥í•  í´ë” ìƒì„±

        attention_maps = torch.stack(attention_maps, dim=1).detach()  # (Batch, Layers, Time Steps, 1)
        batch_size, num_layers, time_steps = attention_maps.shape

        for batch_idx in range(batch_size):  # ğŸ”¥ ë°°ì¹˜ë³„ë¡œ ì €ì¥
            avg_attention = attention_maps[batch_idx].squeeze(-1)  # (Layers, Time Steps)

            fig, axes = plt.subplots(2, 1, figsize=(12, 10), gridspec_kw={'height_ratios': [3, 2]})  # ğŸ”¥ íˆíŠ¸ë§µ: ì„  ê·¸ë˜í”„ = 3:2 ë¹„ìœ¨

            ## ğŸ”¹ (1) ì–´í…ì…˜ ë§µ íˆíŠ¸ë§µ (ìœ„)
            sns.heatmap(avg_attention.cpu().numpy(), cmap="Reds", ax=axes[0], square="auto")
            axes[0].set_xlabel("Time Steps", fontsize=10)
            axes[0].set_ylabel("Layers", fontsize=10)
            axes[0].set_title(f"Attention Map (Sample {batch_idx})", fontsize=12)

            ## ğŸ”¹ (2) ì–´í…ì…˜ ê°’ 1D ì„  ê·¸ë˜í”„ (ì•„ë˜)
            for layer_idx in range(num_layers):
                axes[1].plot(
                    range(time_steps), avg_attention[layer_idx].cpu().numpy(), label=f"Layer {layer_idx}"
                )

            axes[1].set_xlabel("Time Steps", fontsize=10)
            axes[1].set_ylabel("Attention Weight", fontsize=10)
            axes[1].set_title("Layer-wise Attention Distribution over Time", fontsize=12)
            axes[1].legend(loc="upper right", fontsize=8)
            axes[1].grid(True)

            ## ğŸ”¹ ì €ì¥
            save_path = os.path.join(save_dir, f"attn_map_sample_{batch_idx}.png")
            plt.tight_layout()  # ë ˆì´ì•„ì›ƒ ì¡°ì •
            plt.savefig(save_path, dpi=300)  # ğŸ”¥ ì €ì¥
            plt.close()

            print(f"âœ… ì–´í…ì…˜ ë§µ + 1D ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")

class LayerWiseNone(nn.Module):
    def __init__(self, num_layers, mask_ratio=0.0):
        super(LayerWiseNone, self).__init__()
        self.mask_ratio = mask_ratio
        self.num_layers = num_layers
        # ë ˆì´ì–´ë³„ í•™ìŠµ ê°€ì¤‘ì¹˜ (ì´ˆê¸°ê°’ 1)
        self.layer_weights = nn.Parameter(torch.ones(num_layers))

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor or list):
                - [Batch, Layer, Frame, Dim] í˜•íƒœì˜ í…ì„œ
                  (ë ˆì´ì–´ ìˆ˜ = self.num_layers)
                - ë§Œì•½ list í˜•íƒœë¼ë©´, [Batch, Layer, Frame, Dim]ìœ¼ë¡œ stack ì²˜ë¦¬
        Returns:
            torch.Tensor: [Batch, Dim] í˜•íƒœì˜ ìµœì¢… ì„ë² ë”©
        """
        # ë§Œì•½ layer_repsê°€ listë¡œ ë“¤ì–´ì˜¨ë‹¤ë©´ tensorë¡œ ë³€í™˜
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)
        
        # layer_reps.shape: (Batch, num_layers, Frame, Dim)

        # 1) ë ˆì´ì–´ë³„ ê°€ì¤‘ì¹˜ ê³± + í•©ì‚°
        #    layer_weights (num_layers,) â†’ (1, num_layers, 1, 1)
        #    => ë ˆì´ì–´ ì°¨ì›(num_layers)ì— ëŒ€í•´ ë¸Œë¡œë“œìºìŠ¤íŒ… ê³±
        weighted_sum = (
            layer_reps * self.layer_weights.view(1, -1, 1, 1)
        ).sum(dim=1)  # ê²°ê³¼ shape: [Batch, Frame, Dim]

        # 2) í”„ë ˆì„ ì°¨ì›ì— ëŒ€í•´ í‰ê·  í’€ë§ -> [Batch, Dim]
        utterance_level_embedding = weighted_sum.mean(dim=1)

        return utterance_level_embedding

class LayerWiseNoneSelect(nn.Module):
    def __init__(self, num_layers, mask_ratio=0.0):
        super(LayerWiseNoneSelect, self).__init__()
        self.mask_ratio = mask_ratio
        self.num_layers = num_layers
        
        # ë ˆì´ì–´ë³„ í•™ìŠµ ê°€ì¤‘ì¹˜ (ì´ˆê¸°ê°’ 1)
        self.layer_weights = nn.Parameter(torch.ones(num_layers))

        # ë‚´ë¶€ì ìœ¼ë¡œ ì—¬ëŸ¬ ë ˆì´ì–´ ì¸ë±ìŠ¤ ì§€ì • (0-based)
        # ì˜ˆ: [5, 6, 7] â†’ ì‚¬ëŒ ê¸°ì¤€(1-based)ìœ¼ë¡  6,7,8ë²ˆì§¸ ë ˆì´ì–´
        self.selected_layers = [5, 6, 7, 10, 12]

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor or list):
                - [Batch, Layer, Frame, Dim] í˜•íƒœ (num_layers = self.num_layers)
                - listë¼ë©´ stackí•˜ì—¬ ëª¨ì–‘ í†µì¼
        Returns:
            torch.Tensor: [Batch, Dim] í˜•íƒœ ìµœì¢… ì„ë² ë”©
        """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)
        # => (B, num_layers, F, D)

        # (1) ì—¬ëŸ¬ ë ˆì´ì–´ë¥¼ ì„ íƒí•´ì„œ ì¶”ì¶œ
        #     (B, selected_layer_count, F, D) í˜•íƒœ
        selected_out = layer_reps[:, self.selected_layers, :, :]
        
        # (2) ì„ íƒëœ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë§Œ ê°€ì ¸ì˜´
        #     (num_layers,) â†’ (selected_layer_count,)
        selected_weights = self.layer_weights[self.selected_layers]

        # (3) ë ˆì´ì–´ë³„ ê°€ì¤‘ì¹˜ ê³±
        #     selected_out.shape = (B, selected_layer_count, F, D)
        #     selected_weights.shape = (selected_layer_count,)
        #     ë¸Œë¡œë“œìºìŠ¤íŒ… ìœ„í•´ (1, selected_layer_count, 1, 1)
        weighted_sum = (
            selected_out * selected_weights.view(1, -1, 1, 1)
        ).sum(dim=1)  # (B, F, D)

        # (4) í”„ë ˆì„ ì°¨ì› í‰ê·  í’€ë§ â†’ (B, D)
        utterance_level_embedding = weighted_sum.mean(dim=1)

        return utterance_level_embedding

class LayerWiseNoneAttn(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseNoneAttn, self).__init__()
        self.mask_ratio = mask_ratio  # ì—¬ê¸°ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨
        self.num_layers = num_layers

        # (1) ë ˆì´ì–´ë³„ í•™ìŠµ ê°€ì¤‘ì¹˜ (ì´ˆê¸°ê°’ 1)
        self.layer_weights = nn.Parameter(torch.ones(num_layers))

        # (2) ìµœì¢… í”„ë ˆì„ í’€ë§ì„ ìœ„í•œ ì–´í…ì…˜ íŒŒíŠ¸(Attentive Pooling)
        self.att_linear = nn.Linear(input_dim, input_dim, bias=True)    # U^T
        self.att_vector = nn.Linear(input_dim, 1, bias=True)           # v^T

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor or list):
                - [Batch, num_layers, Frame, Dim] í˜•íƒœ í…ì„œ
                - listë¡œ ë“¤ì–´ì˜¤ë©´ stackí•˜ì—¬ ëª¨ì–‘ í†µì¼
        Returns:
            utterance_level_embedding: [Batch, Dim] ìµœì¢… ì„ë² ë”©
        """
        # 0) ë§Œì•½ listë¼ë©´ [Batch, Layer, Frame, Dim]ìœ¼ë¡œ ë³€í™˜
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)
        # => shape: (B, num_layers, Frame, Dim)

        # 1) ë ˆì´ì–´ë³„ ê°€ì¤‘ì¹˜ ê³± + í•©ì‚°
        #    layer_weights: (num_layers,) â†’ (1, num_layers, 1, 1)ë¡œ reshape
        #    ë ˆì´ì–´ ì°¨ì›(num_layers) ê¸°ì¤€ìœ¼ë¡œ í•©ì‚°í•˜ë©´ -> (B, Frame, Dim)
        weighted_sum = (
            layer_reps * self.layer_weights.view(1, -1, 1, 1)
        ).sum(dim=1)  # shape: [Batch, Frame, Dim]

        # 2) ë§ˆì§€ë§‰ í”„ë ˆì„ í’€ë§ì„ ì–´í…í‹°ë¸Œ í’€ë§ìœ¼ë¡œ ìˆ˜í–‰
        #    (B, Frame, Dim) â†’ (B, Dim)
        # 2-1) ì ìˆ˜ ê³„ì‚°: Linear â†’ tanh â†’ Linear
        scores = self.att_linear(weighted_sum)         # (B, F, D)
        scores = torch.tanh(scores)
        scores = self.att_vector(scores).squeeze(-1)   # (B, F)

        # 2-2) Softmaxë¡œ í”„ë ˆì„ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        attention_weights = F.softmax(scores, dim=1)    # (B, F)

        # 2-3) ê°€ì¤‘í•©(Weighted Sum)
        #      (B, F, D) * (B, F) -> (B, D)
        utterance_level_embedding = torch.sum(
            weighted_sum * attention_weights.unsqueeze(-1), 
            dim=1
        )
        # self.save_attention_map(attention_weights)

        return utterance_level_embedding

    def save_attention_map(self, attention_weights):
        """
        ğŸ”¹ ë‹¨ì¼ ì–´í…ì…˜ ë§µì„ ì‹œê°í™”í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜
        Args:
            attention_weights: (Batch, Time Steps)
        """
        save_dir = "after_attn_map"
        os.makedirs(save_dir, exist_ok=True)  # ğŸ”¥ ì €ì¥í•  í´ë” ìƒì„±

        batch_size, time_steps = attention_weights.shape

        for batch_idx in range(batch_size):
            attn_map = attention_weights[batch_idx].cpu().detach().numpy()  # (Time Steps,)

            fig, axes = plt.subplots(2, 1, figsize=(10, 8), gridspec_kw={'height_ratios': [3, 2]})

            ## ğŸ”¹ (1) ì–´í…ì…˜ ë§µ íˆíŠ¸ë§µ (ìœ„)
            sns.heatmap(attn_map[None, :], cmap="Reds", ax=axes[0], cbar=True, xticklabels=50, yticklabels=[])
            axes[0].set_xlabel("Time Steps", fontsize=10)
            axes[0].set_title(f"Attention Map (Sample {batch_idx})", fontsize=12)

            ## ğŸ”¹ (2) ì–´í…ì…˜ ê°’ 1D ì„  ê·¸ë˜í”„ (ì•„ë˜)
            axes[1].plot(range(time_steps), attn_map, label=f"Attention Weight", color='red')
            axes[1].set_xlabel("Time Steps", fontsize=10)
            axes[1].set_ylabel("Attention Weight", fontsize=10)
            axes[1].set_title("Frame-wise Attention Distribution", fontsize=12)
            axes[1].grid(True)

            ## ğŸ”¹ ì €ì¥
            save_path = os.path.join(save_dir, f"attn_map_sample_{batch_idx}.png")
            plt.tight_layout()  # ë ˆì´ì•„ì›ƒ ì¡°ì •
            plt.savefig(save_path, dpi=300)  # ğŸ”¥ ì €ì¥
            plt.close()

            print(f"âœ… ì–´í…ì…˜ ë§µ ì €ì¥ ì™„ë£Œ: {save_path}")

class LayerWiseSelect(nn.Module):
    def __init__(self, input_dim, num_layers, mask_ratio=0.0):
        super(LayerWiseSelect, self).__init__()
        self.mask_ratio = mask_ratio
        self.num_layers = num_layers
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  
        self.aspm_modules = nn.ModuleList(
            [ASPM(input_dim=input_dim) for _ in range(num_layers)]
        )

    def forward(self, layer_reps):
        """
        Args:
            layer_reps (torch.Tensor): [Batch, Layer, Frame, Dim]
        Returns:
            torch.Tensor: [Batch, Dim] ë°œí™”(utterance) ì„ë² ë”©
        """
        # layer_repsê°€ listë¼ë©´, [Batch, Layer, Frame, Dim] í˜•íƒœë¡œ ìŠ¤íƒ
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        # ì˜ˆ: íŠ¹ì • ë ˆì´ì–´ë§Œ ì„ íƒí•´ì„œ ì‚¬ìš©
        # 5 6 7 10 12
        selected_layers = [5, 6, 7]
        # â€» ì‹¤ì œë¡œëŠ” 6, 7, 8 ë ˆì´ì–´ê°€ ì¡´ì¬í•˜ëŠ”ì§€(num_layers > 8) ì²´í¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ.

        # ì„ íƒëœ ë ˆì´ì–´ì˜ masked_output/attention_map ì €ì¥
        selected_outputs = []
        selected_attn_weights = []

        for layer_idx in selected_layers:
            # í•´ë‹¹ ë ˆì´ì–´ ì„ë² ë”©
            layer_output = layer_reps[:, layer_idx, :, :]  # [Batch, Frame, Dim]

            # ë¬´ì‘ìœ„ ë§ˆìŠ¤í‚¹ ASPM ëª¨ë“ˆ ì ìš©
            masked_output, attention_map = self.aspm_modules[layer_idx](
                layer_output, mask_ratio=self.mask_ratio
            )
            selected_outputs.append(masked_output)
            selected_attn_weights.append(attention_map)

        # [Batch, #selected_layers, Frame, Dim] í˜•íƒœë¡œ í•©ì¹˜ê¸°
        selected_outputs = torch.stack(selected_outputs, dim=1)

        # ì„ íƒëœ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë§Œ ê³¨ë¼ì˜´
        selected_layer_weights = self.layer_weights[selected_layers]  
        # [num_layers] â†’ [#selected_layers]

        # ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ê³±í•´ì¤€ ë’¤ sum(ë ˆì´ì–´ ì°¨ì›)
        weighted_outputs = (selected_outputs *
                            selected_layer_weights.view(1, -1, 1, 1)).sum(dim=1)
        # ê²°ê³¼: [Batch, Frame, Dim]
        # ìµœì¢… í”„ë ˆì„ ì°¨ì› í‰ê·  â†’ [Batch, Dim]
        utterance_level_embedding = torch.mean(weighted_outputs, dim=1)

        return utterance_level_embedding

class LayerWiseBaseEpoch(nn.Module):
    def __init__(self, input_dim, num_layers, total_epochs=15, mask_ratio=0.0):
        super(LayerWiseBaseEpoch, self).__init__()
        self.num_layers = num_layers
        self.total_epochs = total_epochs
        self.current_epoch = 0  # ğŸ”¥ í•™ìŠµ ì‹œì‘ ì‹œ epochì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        self.layer_weights = nn.Parameter(torch.ones(num_layers))  # í•™ìŠµ ê°€ëŠ¥í•œ ë ˆì´ì–´ ê°€ì¤‘ì¹˜
        self.aspm_modules = nn.ModuleList([ASPM(input_dim=input_dim) for _ in range(num_layers)])

        # ì´ˆë°˜ì—ëŠ” ë§ˆìŠ¤í‚¹ ì—†ìŒ (0%) â†’ ìµœì¢…ì ìœ¼ë¡œ 50%ê¹Œì§€ ì¦ê°€
        self.initial_mask_ratio = 0.0  # ğŸ”¥ 0% (ì´ˆë°˜ 5 epoch ë™ì•ˆ)
        self.increase_epochs = 10
        self.mask_ratio = mask_ratio      # ğŸ”¥ ìµœì¢… 50%

    def update_epoch(self):
        """ ğŸ”¥ ì™¸ë¶€ì—ì„œ í˜¸ì¶œí•˜ì—¬ í˜„ì¬ epoch ê°’ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜ """
        self.current_epoch = min(self.current_epoch + 1, self.total_epochs)

    def get_mask_ratios(self):
        """ ğŸ”¥ ì´ˆë°˜ 10 epoch ë™ì•ˆ 50%ê¹Œì§€ ì¦ê°€, ì´í›„ ê³ ì • """
        if self.current_epoch < self.increase_epochs:
            # ğŸ”¥ 10 epoch ë™ì•ˆ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€
            progress = self.current_epoch / self.increase_epochs
            mask_ratio = self.initial_mask_ratio + (self.mask_ratio - self.initial_mask_ratio) * progress
        else:
            # ğŸ”¥ 10 epoch ì´í›„ì—ëŠ” 50% ê³ ì •
            mask_ratio = self.mask_ratio
       
        return [mask_ratio] * self.num_layers  # ëª¨ë“  ë ˆì´ì–´ì— ë™ì¼í•œ ë¹„ìœ¨ ì ìš©


    def forward(self, layer_reps):
        """ ğŸ”¥ forwardì—ì„œ epochì„ ë”°ë¡œ ì…ë ¥í•˜ì§€ ì•Šì•„ë„ ë¨ """
        if isinstance(layer_reps, list):
            layer_reps = torch.stack(layer_reps, dim=1)

        batch_size, num_layers, frame_size, dim = layer_reps.shape
        masked_outputs = []

        mask_ratios = self.get_mask_ratios()  # ğŸ”¥ ë‚´ë¶€ì ìœ¼ë¡œ epochì„ ì°¸ì¡°í•˜ì—¬ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ ê³„ì‚°

        
        for layer_idx in range(num_layers):
            layer_output = layer_reps[:, layer_idx, :, :]
            mask_ratio = mask_ratios[layer_idx]

            masked_output = self.aspm_modules[layer_idx](layer_output, mask_ratio=self.mask_ratio)

            masked_outputs.append(masked_output)

        masked_outputs = torch.stack(masked_outputs, dim=1)

        weighted_sum = (masked_outputs * self.layer_weights.view(1, -1, 1, 1)).sum(dim=1)

        utterance_level_embedding = torch.mean(weighted_sum, dim=1)

        return utterance_level_embedding

class spk_extractor(nn.Module):
    def __init__(self, mask_ratio = 0.0, **kwargs):
        super(spk_extractor, self).__init__()

        print(f"ğŸ”¹ [spk_extractor] Initialized with mask_ratio: {mask_ratio}")

        checkpoint = torch.load('baseline/pretrained/WavLM-Base+.pt')
        # print("Pre-trained Model: {}".format(kwargs['pretrained_model_path']))
        # checkpoint = torch.load(kwargs['pretrained_model_path'])
        cfg = WavLMConfig(checkpoint['cfg'])
        self.model = WavLM(cfg)
        self.loadParameters(checkpoint['model'])
        # Add LayerWiseASP module
        self.weighted_pooling = LayerWiseBase(input_dim=768, num_layers=13, mask_ratio=mask_ratio)
        self.final_linear = nn.Linear(768, 256, bias=True)  # Final projection layer


    def forward(self, wav_and_flag):
        
        x = wav_and_flag[0]

        cnn_outs, layer_results =  self.model.extract_features(x, output_layer=13)
        layer_reps = [x.transpose(0, 1) for x, _ in layer_results]
        # x = torch.stack(layer_reps).transpose(0,-1).transpose(0,1)

        utterance_level_embedding = self.weighted_pooling(layer_reps)

        final_output = self.final_linear(utterance_level_embedding) 

        # out = self.backend(x)
        return final_output

    def loadParameters(self, param):

        self_state = self.model.state_dict();
        loaded_state = param

        for name, param in loaded_state.items():
            origname = name;
            

            if name not in self_state:
                # print("%s is not in the model."%origname);
                continue;

            if self_state[name].size() != loaded_state[origname].size():
                print("Wrong parameter length: %s, model: %s, loaded: %s"%(origname, self_state[name].size(), loaded_state[origname].size()));
                continue;

            self_state[name].copy_(param);


def MainModel(**kwargs):
    mask_ratio = kwargs.get("mask_ratio", 0.0)
    print(f"ğŸ”¹ [MainModel] Passing mask_ratio: {mask_ratio}")
    model = spk_extractor(**kwargs)
    return model

if __name__ == "__main__":
        
    from torchinfo import summary

    # Model initialization
    model = spk_extractor()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Dummy input for testing, shape (1, 48240)
    dummy_input = torch.randn(1, 40, 48240).to(device)

    # Correct input format for the forward method
    summary(model, input_data=(dummy_input))
